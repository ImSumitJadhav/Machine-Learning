{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeL7FKXzynlFv+RI5zGJQl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImSumitJadhav/Machine-Learning/blob/main/T7_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP**"
      ],
      "metadata": {
        "id": "aa6alrdQlmxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Tokenazation**"
      ],
      "metadata": {
        "id": "WDGzej7ukl69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\""
      ],
      "metadata": {
        "id": "axKzLFNwgOqZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "tK0Zh40HdHgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90dff36-29cc-477d-d30e-301f7f6f5c03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a1=nltk.sent_tokenize(\"The coefficient estimates for Ordinary Least Squares rely on the independence of the features.\")"
      ],
      "metadata": {
        "id": "kiw98X-Ccb86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a2=nltk.word_tokenize(\"The coefficient estimates for Ordinary Least Squares rely on the independence of the features.\")"
      ],
      "metadata": {
        "id": "3yYhBEpLf7oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a3=nltk.wordpunct_tokenize(\"The coefficient estimates for Ordinary Least Squares rely on the independence of the features.\")"
      ],
      "metadata": {
        "id": "HniEVZXRgAcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "6kzr0mS9cb6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "id": "_ipzO5jlcbwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "regexp_tokenize(text, \"[\\w']+\")\n"
      ],
      "metadata": {
        "id": "jza6PKx1cbj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Stemming**"
      ],
      "metadata": {
        "id": "VIEBG2XHkpWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "bNNlPGPmkpHV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Some Stemming algorithms are**\n",
        " \n",
        "\n",
        "**Porter’s Stemmer algorithm**\n",
        "\n",
        "It is one of the most popular stemming methods proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity. The main applications of Porter Stemmer include data mining and Information retrieval. However, its applications are only limited to English words. Also, the group of stems is mapped on to the same stem and the output stem is not necessarily a meaningful word. The algorithms are fairly lengthy in nature and are known to be the oldest stemmer.\n",
        "Example: EED -> EE means “if the word has at least one vowel and consonant plus EED ending, change the ending to EE” as ‘agreed’ becomes ‘agree’. \n",
        " \n",
        "Advantage: It produces the best output as compared to other stemmers and it has less error rate.\n",
        "\n",
        "Limitation:  Morphological variants produced are not always real words.\n",
        "\n",
        "**Lovins Stemmer**\n",
        "\n",
        "It is proposed by Lovins in 1968, that removes the longest suffix from a word then the word is recorded to convert this stem into valid words. \n",
        "Example: sitting -> sitt -> sit \n",
        " \n",
        "Advantage: It is fast and handles irregular plurals like 'teeth' and 'tooth' etc.\n",
        "\n",
        "Limitation: It is time consuming and frequently fails to form words from stem.\n",
        "\n",
        "**Dawson Stemmer**\n",
        "\n",
        "It is an extension of Lovins stemmer in which suffixes are stored in the reversed order indexed by their length and last letter. \n",
        " \n",
        "Advantage: It is fast in execution and covers more suffices.\n",
        "\n",
        "Limitation: It is very complex to implement.\n",
        " \n",
        "\n",
        "**Krovetz Stemmer**\n",
        "\n",
        "It was proposed in 1993 by Robert Krovetz. Following are the steps: \n",
        "1) Convert the plural form of a word to its singular form. \n",
        "2) Convert the past tense of a word to its present tense and remove the suffix ‘ing’. \n",
        "Example: ‘children’ -> ‘child’ \n",
        " \n",
        "Advantage: It is light in nature and can be used as pre-stemmer for other stemmers.\n",
        "\n",
        "Limitation: It is inefficient in case of large documents.\n",
        "\n",
        "**Xerox Stemmer** \n",
        "\n",
        "Example: \n",
        "‘children’ -> ‘child’\n",
        "‘understood’ -> ‘understand’\n",
        "‘whom’ -> ‘who’\n",
        "‘best’ -> ‘good’\n",
        "N-Gram Stemmer \n",
        "An n-gram is a set of n consecutive characters extracted from a word in which similar words will have a high proportion of n-grams in common. \n",
        "\n",
        "Example: ‘INTRODUCTIONS’ for n=2 becomes : *I, IN, NT, TR, RO, OD, DU, UC, CT, TI, IO, ON, NS, S* \n",
        " \n",
        "Advantage: It is based on string comparisons and it is language dependent.\n",
        "\n",
        "Limitation: It requires space to create and index the n-grams and it is not time efficient.\n",
        "\n",
        "**Snowball Stemmer:**\n",
        "\n",
        "When compared to the Porter Stemmer, the Snowball Stemmer can map non-English words too. Since it supports other languages the Snowball Stemmers can be called a multi-lingual stemmer. The Snowball stemmers are also imported from the nltk package. This stemmer is based on a programming language called ‘Snowball’ that processes small strings and is the most widely used stemmer. The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer. Because of the improvements added when compared to the Porter Stemmer, the Snowball stemmer is having greater computational speed. \n",
        "\n",
        "**Lancaster Stemmer:**\n",
        "\n",
        "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm. "
      ],
      "metadata": {
        "id": "s7f_KiconZzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Lemmitizing**"
      ],
      "metadata": {
        "id": "oK3BEGTDoCYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "DZ6kGM0XoCE0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.Removing Stopwords**"
      ],
      "metadata": {
        "id": "yFSQYeGXrHHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "UYia3zE5rXWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.Text to Vectors**"
      ],
      "metadata": {
        "id": "x2dh_jRCrjDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "DzRApYz8rrmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "sTq8v5v1ti7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Word2Vec\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "Op2KEIfMtrlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. GloVe\n",
        "from gensim.models import GloVe"
      ],
      "metadata": {
        "id": "bqbPLoSIt67G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.Similarity Algorithm**"
      ],
      "metadata": {
        "id": "1xD3a6R0ve5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity    \n",
        "\n",
        "#used to find similar vectors\n",
        "sim=cosine_similarity()\n"
      ],
      "metadata": {
        "id": "i-Sv6Ge6vitT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.Remove Punctuations**"
      ],
      "metadata": {
        "id": "OUOm-xZav9rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "jIt6SG_kwBW7"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}